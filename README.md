**Neural Network Approaches to Parity and Natural Language Translation**

This project explores the application of sequence-to-sequence neural architectures â€” SimpleRNN, LSTM, and Transformer â€” to two distinct tasks:

- **Parity prediction** (symbolic sequence modeling)

- **Englishâ€“Portuguese translation** (natural language processing)

ðŸš€ Overview
The goal was to evaluate how well different neural architectures capture dependencies in sequences â€” both artificial (parity) and natural language. The models were trained and tested on:

- Synthetic binary strings for parity learning

- Real-world Englishâ€“Portuguese sentence pairs from a parallel corpus

ðŸ§  Architectures Implemented
- **SimpleRNN:** Basic recurrent model with limited ability to handle long-term dependencies.

- **LSTM:** Recurrent model designed to better preserve long-range context via gating mechanisms.

- **Transformer:** Attention-based architecture capable of capturing complex relationships without recurrence.

ðŸ“Š Results Summary
| Task                  | SimpleRNN Accuracy | LSTM Accuracy | Transformer Accuracy |
| --------------------- | ------------------ | ------------- | -------------------- |
| **Parity Prediction** | \~57%              | \~90%         | **\~99.7%**          |
| **Translation (NLP)** | \~24%              | \~33%         | **\~41.2%**          |

The **Transformer** consistently outperformed both SimpleRNN and LSTM across tasks, highlighting its effectiveness in capturing both symbolic logic and natural language structure.

ðŸ”§ Technical Details
- Framework: TensorFlow / Keras

- Data:

     - Parity: Generated binary sequences of varying lengths.

     - Translation: [Englishâ€“Portuguese parallel dataset](https://raw.githubusercontent.com/luisroque/deep-learning-articles/main/data/eng-por.txt)


- Custom Loss & Accuracy Functions: Masked sparse categorical loss and accuracy were implemented to ignore padding tokens during training.

- Training:
  
      -All models trained for 200â€“1000 epochs using the Nadam optimizer.
      -Validation split used to monitor generalization performance.


âœ… Key Achievements
- Successfully demonstrated the superiority of Transformer models in both symbolic and language-based sequence learning.

- Validated that attention mechanisms enable better performance in tasks involving complex sequential logic and contextual meaning.

- Applied NLP techniques to real-world translation data, showing measurable improvement over traditional recurrent models.
